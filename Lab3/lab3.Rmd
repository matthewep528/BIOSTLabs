---
title: "M3 Lab"
author: "Matthew Pr√¶stgaard"
date: "Spring 2024"
output:
  rmdformats::downcute:
    self_contained: true
    default_style: "dark"
    downcute_theme: "default"
    code_folding: "show"
  svglite:
    fig.retina: 2
---

# Libraries and Functions

```{r, echo = FALSE, message = FALSE, warning = FALSE}
require(knitr)
knitr::opts_chunk$set(error = TRUE)
```

```{r, message = FALSE}
library(tidyverse)
library(here)
library(car)
library(psych)
library(MASS)
library(lspline)
library(splines)
library(readxl)
library(glmnet)
library(olsrr)
library(jtools)
library(ggthemr)
ggthemr("flat dark")
```

```{r}
# ggplotRegression
# use to plot linear model with summary statistics and error
# Usage: ggplotRegression(data)
ggRegression <- function(fit, title = "title") {
    require(ggplot2)

    ggplot(fit$model, aes_string(x = names(fit$model)[2], y = names(fit$model)[1])) +
        geom_point() +
        stat_smooth(
            method = "lm",
            geom = "smooth"
        ) +
        labs(
            title = title,
            subtitle = paste(
                "Adj R2 = ", signif(summary(fit)$adj.r.squared, 5),
                "Intercept =", signif(fit$coef[[1]], 5),
                " Slope =", signif(fit$coef[[2]], 5),
                " P =", signif(summary(fit)$coef[2, 4], 5)
            )
        )
}
```

```{r, message = FALSE}
# Example usage:
# fit <- lm(mpg ~ disp + hp, data = mtcars)
# ggResid(fit)

ggResid <- function(fit) {
    # Extract fitted values and residuals from the linear model
    fitted_values <- fit$fitted.values
    residuals <- fit$residuals

    # Create a data frame for the residuals vs. fitted values plot
    resid_data <- data.frame(Fitted = fitted_values, Residuals = residuals)

    # Create the plot
    ggplot(resid_data, aes(x = Fitted, y = Residuals)) +
        geom_point() +
        geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
        labs(
            title = "Residuals vs. Fitted Values",
            x = "Fitted Values",
            y = "Residuals"
        )
}
```

# Part 1: Blood Toluene
## Data Management

We will continue using the same data from the module 2 lab for the first part of this lab.

From last time: 

The following data come from a study of the impacts of toluene exposure through inhalation on toluene concentration in the blood.
In the study, 60 rats were exposed to differing levels of toluene inhalation,
from 11.34 to 1744.7 parts per million (ppm) (newppm) for a duration of 3 hours.
Blood levels were measured (bloodtol) following exposure measured in mg/L.
Other variables measured were weight in grams (weight), age in days (age), and snout size as either short (snoutsize=1) or long (snoutsize=2).

We will use these data to model the relationship between toluene exposure and blood concentration, possibly adjusting for other variables.



```{r, message = FALSE}
# read in data for part 1 (blood toluene data, same as lab 2)
here(".lintr")

data <- read_csv(here("Lab3", "hwdata2.csv")) %>%
    mutate(
        snout_f = factor(snoutsize - 1,
            levels = c(0, 1),
            labels = c("short", "long")
        ),
        snoutsize = as.factor(snoutsize)
    )

kable(summary(data))
```


## 1. SLR model

__Fit a simple linear model with bloodtol as the dependent variable and just newppm as the independent variable.__
__Create a components plus residuals plot for this model. Do you notice anything problematic in this plot?__

```{r, warning = FALSE}
# linear model
lmTolPPM <- lm(bloodtol ~ newppm, data = data)

jtools::summ(lmTolPPM)

# C+R plot
pairs.panels(data[, c("bloodtol", "newppm")],
    ellipses = FALSE, density = FALSE, hist.col = "deepskyblue"
)
crPlots(lmTolPPM)
ggRegression(lmTolPPM, title = "Plot of Blood Toluene by Toluene Exposure (ppm)")
ggResid(lmTolPPM)
```

Comment on plot: The residuals are not uniformly distributed around 0 and instead seem to follow a pattern.


## 2. Fit a LOESS curve

__Fit a LOESS curve over the scatterplot of blood toluene (bloodtol) on toluene exposure (newppm).__
__Try three different values of span (the parameter that controls how smooth the curve is): 0.5, 0.75, and 2. Comment on what you see.__

```{r}
# loess curve plot
ggplot(data, aes(x = newppm, y = bloodtol)) +
    geom_point() +
    geom_smooth(span = 0.5) +
    labs(
        title = "LOESS Curve (0.5)"
    )

ggplot(data, aes(x = newppm, y = bloodtol)) +
    geom_point() +
    geom_smooth(span = 0.75) +
    labs(
        title = "LOESS Curve (0.75)"
    )

ggplot(data, aes(x = newppm, y = bloodtol)) +
    geom_point() +
    geom_smooth(span = 2) +
    labs(
        title = "LOESS Curve (2)"
    )
# recall that you can use geom_smooth(method = "loess", span = __) to get a loess curve over a scatterplot (made with geom_point() in ggplot)
```

Comment on what you see:
Especially in the span = 0.5 plot, blood toluene appears to be level for a bit before rising rapidly, leveling off,
rising again, and slightly dropping/levelling again at higher values of newppm.

## 3. Linear spline model

__Fit a model with linear splines using knots at newppm values of 125, 375, and 625.__
__Use marginal coefficients for the spline terms in this model. Make a plot showing the model fit.__


```{r}
# linear spline plot
# recall that you can use
# geom_smooth(method = "lm", formula = y ~ lspline(x, knots = c(knot1, knot2, etc...)))
# reminder that here we don't acually substitutde in variable names for x and y
# (they're taken from the x and y variables oyu define in the main aes() arguments when you made your ggplot plot)
ggplot(data, aes(x = newppm, y = bloodtol)) +
    geom_point() +
    geom_smooth(method = "lm", formula = y ~ lspline(x, knots = c(125, 375, 625), marginal = TRUE)) +
    labs(
        title = "Linear Spline Plot (knots 125, 375, and 625), marginal = true"
    )


# fit linear spline model with marginal coefficients
# reminder: syntax to make linear splines using `lspline` package is
# lspline(variable_name, knots = c(knot1, knot2, ...), marginal = T/F)
tolPPMSpline1 <- lm(bloodtol ~ lspline(newppm, knots = c(125, 375, 625), marginal = TRUE), data = data)
summ(tolPPMSpline1)

# show output
```

## 4. Marginal vs not marginal splines

__Fit a second model without marginal coefficients. What do you notice about these coefficients compared to the previous model?__

```{r}
# linear splines without marginal = T (with marginal = F)
ggplot(data, aes(x = newppm, y = bloodtol)) +
    geom_point() +
    geom_smooth(method = "lm", formula = y ~ lspline(x, knots = c(125, 375, 625), marginal = FALSE)) +
    labs(
        title = "Linear Spline Plot (knots 125, 375, and 625), marginal = false"
    )

tolPPMSpline2 <- lm(bloodtol ~ lspline(newppm, knots = c(125, 375, 625), marginal = FALSE), data = data)
summ(tolPPMSpline2)
```

The marginal model shows cumulative effects: the second segment coefficient decreases (-0.05488), and the third increases (0.06800),
with a sharp decrease in the fourth (-0.10376). The non-marginal model, focusing on segment-specific changes,
shows a more gradual increase in the second (0.01796) and third segments (0.08597), with a milder decrease in the fourth (-0.01779). 

Both models have an R-squared of 0.82, showing a good fit. 

## 5. Estimating slope at a particualr point


__Based on either of the spline models above, what is the estimated slope when the newppm value is 300 (newppm = 300)?__  

Answer here: **13.81767**  

## 6. Natural splines

__Fit a model with natural splines with 4 knots (you can let R pick the location of the knots by creating natural splines with 5 degrees of freedom).__
__Make a plot showing the fit of this model and comment on the fit (visually).__

```{r}
# natural spline plot
# recall that you can use
# geom_smooth(method = "lm", formula = y ~ ns(x, df = ?))
ggplot(data, aes(x = newppm, y = bloodtol)) +
    geom_point() +
    geom_smooth(method = "lm", formula = y ~ ns(x, df = 5)) +
    labs(title = "Fit of Natural Spline Model", x = "New PPM", y = "Blood Toluene")


# fit natural spline model with marginal coefficients
# reminder: syntax to make natural splines using `splines` package is
# ns(variable_name, df = # knots + 1)
# you could also specify the location of the knots, but specifying df lets R choose them for you
nSpline <- lm(bloodtol ~ ns(newppm, df = 5), data = data)

# model output
summ(nSpline)
```

Comment on the fit of the model:
The adjusted R2 of 0.81 shows a very good fit.
The coefficients for the spline terms ns(newppm, df=5)3, ns(newppm, df=5)4, and ns(newppm, df=5)5 are statistically significant,
suggesting that these spline components meaningfully contribute to modeling the relationship between newppm and bloodtol. 

## 7. Residual plots for the models fit so far

__Make residual plots for each of the models (you can just choose one parameterization of the linear spline model) above and comment on any differences you see.__

```{r}
# plots for linear model
ggResid(lmTolPPM)

# plots for linear spline model
ggResid(tolPPMSpline1)

# plots for natural spline model
ggResid(nSpline)
```

Comments:
The plots all show signs of patterns in the residuals, but the second and third plots show more randomness in the distribution of residuals than the first.

## 8. Test for linearity

__Using the linear spline and natural spline models, run tests for linearity. Do we have evidence of non-linearity?__

```{r}
# test for linearity (linear spline model)
anova(lmTolPPM, tolPPMSpline1)
# test for linearity (natural spline model)
anova(lmTolPPM, nSpline)
```

Interpretation of output:
The ANOVA results for both the linear spline model and the natural spline model compared to the linear model show highly significant p-values (***), indicating strong evidence against the null hypothesis of linearity.
This suggests that both spline models provide a significantly better fit to the data than the linear model, indicating the presence of non-linearity


## 9. Model fit based on various metrics

__Compare the fits of the linear, linear spline, and natural spline models.__
__In particular, compare the $R^2$/adjusted $R^2$ values, mean squared errors, AICs, and BICs. Which do you prefer? Justify your answer.__

```{r}
summ(lmTolPPM)
summ(tolPPMSpline1)
summ(nSpline)
```

Comparison and your preference:
The spline models both show a better fit based on the adjusted R2 values. Based on this, I prefer the linear spline model as it is simpler than the natural spline model (fewer knots),
which, I believe, would be good in order to prevent the models from potentially becoming too complex and inapplicable to the real world.

# Part 2: NHANES Study

The following data come from the NHANES study. We are interested in estimating ferritin levels based on a handful of covariates in the data.

## Data management

```{r}
# load in data (change as needed to a file path that works on your device)
nhanes <- read_csv(here("Lab3", "nhanes_subset.csv"))
```

## 1. Full model:

__Fit a multiple linear regression to estimate ferritin with all predictors in the data set.__
__Check the residual plots and construct component plus residual plots and check for influential points.__
__Do you think any data transformation might be helpful here?__

```{r}
# fit max model
nhanesFull <- lm(ferritin ~ ., data = nhanes)
summ(nhanesFull)

# check model fit and check for influential points
# residual plots
ggResid(nhanesFull)

# C+R plots
crPlots(nhanesFull)

nhanes2 <- nhanes
```

Describe what you see and if you think any transformation(s) may be helpful:
The model shows a modest adjusted R2 of 0.18, with the variables sbp, bmi, height, weight, age, and sex being statistically significant predictors.
The residuals plot appears to show a pattern, indicating that a transformation could be appropriate.

# NOTE: Moving this part here since the code would break when it was run after the later chunks

```{r}
set.seed(123) # for cross-validation later



# Backward selection
backward_model <- step(nhanesFull, direction = "backward")

# Preparing data for glmnet
X <- model.matrix(~ . - 1, data = nhanes2[, -1]) # Exclude the intercept and response variable
Y <- nhanes2$ferritin

# Fit Lasso
cv.lasso <- cv.glmnet(X, Y, alpha = 1)
lasso_coefs <- coef(cv.lasso, s = "lambda.min")

# Fit Ridge
cv.ridge <- cv.glmnet(X, Y, alpha = 0)
ridge_coefs <- coef(cv.ridge, s = "lambda.min")

# Fit Elastic Net
cv.elasticnet <- cv.glmnet(X, Y, alpha = 0.5)
elasticnet_coefs <- coef(cv.elasticnet, s = "lambda.min")
```

```{r}
print(lasso_coefs)
print(ridge_coefs)
print(elasticnet_coefs)


# remember that the cv.glmnet() function needs separate X (predictor) and Y (outcome) matrices
# for example, since I called my model `maxmodel_log` I would do
# X = model.matrix(maxmodel_log)[,-1]
# Y = maxmodel_log$model$log_ferritin

# fit lasso (you'll need to choose the appropriate alpha value)



# fit ridge (you'll need to choose the appropriate alpha value)



# elastic net (use alpha = 0.5 for this)


# compare the coefficients/variables chosen
# for a model named `my.model` fit with cv.glmnet
# you can use coef(my.model, s = my.model$lambda.min) to grab the coefficients
# if you use ols_step_backward_p the final model is stored under `my.model`$model
```

## Transforming Variables

(no points here, I've included the code for you)

__Regardless of your answer above, let's go ahead and log-transform a few of the variables that had skewed distributions so they don't have such high leverage. I've included the code for you below and dropped the original variables from the data so we don't have redundancies for the upcoming sections and refit the model from above with the log-transformed data. Notice that the log-transformation seems to have helped__

Note: you don't have to write any code here, just run it so you're working with the log-transformed data.

```{r}
# log transforming and dropping un-transformed values
nhanes <- nhanes %>%
    mutate(
        log_creatinine_urine = log(creatinine_urine),
        log_insulin = log(insulin),
        log_ferritin = log(ferritin),
        log_ghb = log(glycohemoglobin)
    ) %>%
    dplyr::select(-c(creatinine_urine, insulin, ferritin, glycohemoglobin)) # remove original variables

# refit model with all predictors
maxmodel_log <- lm(log_ferritin ~ ., data = nhanes)

# check model fit and check for influential points
# residual plots
plot(maxmodel_log, which = 1:2)
# C+R plots
crPlots(maxmodel_log, ylab = "C + R (ferritin)")
```

Nice! That seems to have helped. 

## 2. VIF

__Check for collinearity in the model that you just fit with all predictors included . Do you see any problematic collinearity?__

```{r}
# check for collinearity
vif_values <- vif(nhanesFull)
print(vif_values)

high_vif <- vif_values[vif_values > 5]
print(high_vif)
```

Comments:
Based on a slightly arbitrary cutoff of 5, the variables weight, height, bmi, arm_circ, and waist_circ indicate potential collinearity.

## 3. Variable selection (see note above)

__Run backward selection as well as cross-validated (10-fold) lasso regression, ridge regression, and elastic net to choose a model from the full set of covariates used in the previous section. Compare the set of variables and coefficients you end up with from each method of variable selection. What do you notice?__

**See the above section about code breaking. The comments are still in this section, however.**

*Note: The function I taught you for backward selection doesn't have any cross-validation capabilities built-in. There are ways to do cross-validated forward/backward selection(either by hand or with other R packages) but for now we won't. Also note that this is part of the reason why the step function uses AIC to pick variables (trying to control for overfitting)*



Comment on similarities/differences between the final models from the various selection processes.
Based on the coefficients from the various selection processes, the lasso and elastic net models appear to have produced sparser models,
 selecting fewer variables by shrinking some coefficients to zero. In contrast, the ridge regression retains all predictors but with smaller coefficient values, 
 suggesting a more conservative shrinkage approach without actual variable elimination. 

## 4. VIFs and model selection

__Think back to the VIF values you got from the full model. What do you notice about the variables retained in these various models with respect to our collinearity results from above? Why do you think that is?__

Answer here:
The regularization methods, especially lasso and elastic net, tend to exclude or significantly shrink the coefficients of variables with high multicollinearity
 (indicated by high VIF values). This suggests that these methods effectively address collinearity by either removing collinear variables (lasso and elastic net)
  or by reducing their influence on the model (ridge).

## 5. Compare model fit

__Compare the model fit from the final models from lasso, ridge, and elastic net. Which model would you prefer?__

Note: I didn't include backward selection for this question because we didn't use cross-validation for that method, so that wouldn't be a very fair comparison.

```{r}
# For Lasso
lasso_best_index <- cv.lasso$index[1]
lasso_best_mse <- cv.lasso$cvm[lasso_best_index]
lasso_best_r2 <- cv.lasso$glmnet.fit$dev.ratio[lasso_best_index]

# For Ridge
ridge_best_index <- cv.ridge$index[1]
ridge_best_mse <- cv.ridge$cvm[ridge_best_index]
ridge_best_r2 <- cv.ridge$glmnet.fit$dev.ratio[ridge_best_index]

# For Elastic Net
elasticnet_best_index <- cv.elasticnet$index[1]
elasticnet_best_mse <- cv.elasticnet$cvm[elasticnet_best_index]
elasticnet_best_r2 <- cv.elasticnet$glmnet.fit$dev.ratio[elasticnet_best_index]

# Compare MSEs
print(paste("Lasso MSE:", lasso_best_mse))
print(paste("Ridge MSE:", ridge_best_mse))
print(paste("Elastic Net MSE:", elasticnet_best_mse))

# Compare R2 values
print(paste("Lasso R2:", lasso_best_r2))
print(paste("Ridge R2:", ridge_best_r2))
print(paste("Elastic Net R2:", elasticnet_best_r2))
```

Your model choice and rationale:  
The R2 values all appear very similar. The lasso and elastic net MSEs are very similar as well. 
Between Lasso and Elastic Net, the Lasso has a slightly lower MSE, suggesting it may be the best model in terms of prediction error.

## 6. Cross-validation

__What was the purpose of using cross-validation in these models?__

Cross-validation helped us identify which variables could potentially be contributing to overfitting.



```{r}
sessionInfo()
```

